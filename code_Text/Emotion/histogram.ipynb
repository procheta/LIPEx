{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: emotion/split\n",
      "Found cached dataset emotion (/home/hongbo/.cache/huggingface/datasets/emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967913a621e746c0bc9c8d08466a71dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total texts in train dataset: 1327\n",
      "dataset_label: ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'] <class 'list'>\n",
      "GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.utils import Bunch\n",
    "import random\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append('../../')\n",
    "from lime_new.lime_text import LimeTextExplainer\n",
    "\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "\n",
    "\n",
    "emotion = load_dataset('emotion')\n",
    "\n",
    "# test_text, test_label = emotion['test']['text'], emotion['test']['label']\n",
    "# new_test_text, new_text_label = [], []\n",
    "# for i in range(len(test_text)):\n",
    "#     if len(set(test_text[i].split(' '))) > 30:\n",
    "#         new_test_text.append(test_text[i].lower())\n",
    "#         new_text_label.append(test_label[i])\n",
    "# assert len(new_test_text) == len(new_text_label)\n",
    "# new_test = Bunch(data=new_test_text, target=np.array(new_text_label))\n",
    "# print('Total texts in test dataset:',len(new_test.data)) # 165\n",
    "\n",
    "\n",
    "train_text, train_label = emotion['train']['text'], emotion['train']['label']\n",
    "new_train_text, new_train_label = [], []\n",
    "for i in range(len(train_text)):\n",
    "    if len(set(train_text[i].split(' '))) > 30: \n",
    "        new_train_text.append(train_text[i].lower())\n",
    "        new_train_label.append(train_label[i])\n",
    "assert len(new_train_text) == len(new_train_label)\n",
    "new_train = Bunch(data=new_train_text, target=np.array(new_train_label))\n",
    "print('Total texts in train dataset:',len(new_train.data)) #1327\n",
    "\n",
    "\n",
    "# label_names = emotion[\"test\"].features['label'].names\n",
    "label_names = emotion[\"train\"].features['label'].names\n",
    "print('dataset_label:',label_names,type(label_names))\n",
    "N_labels = len(label_names)\n",
    "\n",
    "PATH = '/mnt/b432dc15-0b9a-4f76-9076-5bf99fe91d74/Hongbo/LIPEx/code_Text/Emotion'\n",
    "PRETRAINED_LM = \"bert-base-uncased\"\n",
    "\n",
    "config = BertConfig.from_pretrained(PRETRAINED_LM)\n",
    "config.num_labels = N_labels\n",
    "model = BertForSequenceClassification(config)\n",
    "model.load_state_dict(torch.load(PATH+'/model-25.pt'))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('GPU:',torch.cuda.get_device_name(device=device))\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_LM)\n",
    "\n",
    "def encode(docs):\n",
    "    '''\n",
    "    This function takes list of texts and returns input_ids and attention_mask of texts\n",
    "    '''\n",
    "    encoded_dict = tokenizer(docs, add_special_tokens=True, max_length=128, padding=True, return_attention_mask=True, truncation=True, return_tensors='pt') # max_length to be defined\n",
    "\n",
    "    input_ids = encoded_dict['input_ids']\n",
    "    attention_masks = encoded_dict['attention_mask']\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "# Wrap tokenizer and model for LIME\n",
    "class pipeline(object):\n",
    "    def __init__(self, model, encoder): \n",
    "        self.model = model.to(device)\n",
    "        self.encoder = encoder\n",
    "        self.model.eval()\n",
    "    \n",
    "    def predict(self, text, batch_size=128): #batch_size to be defined\n",
    "        num_batches = int(len(text)/batch_size) if len(text)%batch_size == 0 else int(len(text)/batch_size)+1\n",
    "        out = []\n",
    "        for num in range(num_batches):\n",
    "            batch_text = text[num*batch_size:(num+1)*batch_size]\n",
    "\n",
    "            batch_input_ids,batch_attention_mask = self.encoder(batch_text)\n",
    "\n",
    "            batch_input_ids = batch_input_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "\n",
    "            batch_output = self.model(input_ids=batch_input_ids, attention_mask=batch_attention_mask).logits # (batch_size, num_class)\n",
    "            batch_out = batch_output.softmax(dim=-1).cpu().detach().tolist() # (batch_size, num_class)\n",
    "            out += batch_out\n",
    "        return np.array(out)\n",
    "\n",
    "c = pipeline(model,encoder=encode)\n",
    "\n",
    "# distance metrics of two distributions\n",
    "def TV(p, q):\n",
    "        TotalVar_D = torch.mean(0.5 * (torch.sum(torch.abs(p-q), dim=-1))) \n",
    "        return TotalVar_D.item()\n",
    "\n",
    "# def f_re_pred_dis(idx,explainer_TopK_words):\n",
    "#    # Re-Prediction of predictor\n",
    "#     processed_text  = new_train.data[idx]\n",
    "#     for word in explainer_TopK_words:\n",
    "#         processed_text = processed_text.replace(word,'')\n",
    "#     f_output = c.predict([processed_text])\n",
    "#     return torch.from_numpy(f_output)\n",
    "\n",
    "# def exp_re_pred_dis(sample_data,explainer_TopK_features_idx,explainer_exp,explainer_model):\n",
    "#     # Re-Prediction of explainer\n",
    "#     new_sample_data = sample_data[0].copy()\n",
    "#     new_sample_data[explainer_TopK_features_idx] = 0\n",
    "#     new_sample_data = torch.from_numpy(new_sample_data[explainer_exp.used_features]).float()\n",
    "#     explainer_repred = explainer_model.predict(new_sample_data.reshape(1,-1))\n",
    "#     return explainer_repred\n",
    "\n",
    "def random_chose_documents(dataset,num_of_documents):\n",
    "    random_documents_idx = random.sample(range(0, len(dataset.data)), num_of_documents)\n",
    "    return random_documents_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 249 , f_Predicted label: anger , True label: anger\n",
      "Break here. (cnt > 1000))\n",
      "TVLoss: Step [3329], Training Loss: 0.0493\n",
      "TV(f(s), LIPEx(s)) 0.03056283388286829\n",
      "Document ID: 659 , f_Predicted label: joy , True label: joy\n",
      "Break here. (cnt > 1000))\n",
      "TVLoss: Step [4250], Training Loss: 0.0297\n",
      "TV(f(s), LIPEx(s)) 0.004571334226056933\n",
      "TV(f(s), LIPEx(s)) = [0.03056283388286829, 0.004571334226056933]\n",
      "len(TV_s)) = 2\n",
      "Avg(TV(f(s), LIPEx(s))) = 0.01756708405446261\n",
      "Std(TV(f(s), LIPEx(s))) = 0.012995749828405678\n"
     ]
    }
   ],
   "source": [
    "TopK = 5 \n",
    "number_of_features_to_remove = [1,2,3,4,5]\n",
    "\n",
    "union_num = 5\n",
    "num_samples = 1000 # to be defined\n",
    "num_of_test_documents = 2 # to be defined\n",
    "\n",
    "TV_s = [] \n",
    "\n",
    "for idx in random_chose_documents(new_train,num_of_test_documents):\n",
    "   \n",
    "    f_output = c.predict([new_train.data[idx]]) # (num_text, num_class)\n",
    "    _, predicted = torch.max(torch.tensor(f_output), 1)\n",
    "    pred_label= predicted.detach().numpy()[0] #  get the Predicted top label index\n",
    "\n",
    "    print('Document ID:',idx,',','f_Predicted label:',label_names[pred_label],',','True label:',label_names[new_train.target[idx]]) \n",
    "    \n",
    "    #------------------------Below for LIME and LIPEx ------------------------#\n",
    "    explainer = LimeTextExplainer(class_names=label_names,random_state=42)\n",
    "    # sample perturbation data, features2use: Union Set \n",
    "    sample_data, sample_labels, sample_distances, sample_weights, features2use = explainer.sample_data_and_features(new_train.data[idx], c.predict, num_features=union_num, num_samples=num_samples)\n",
    "\n",
    "    LIPEx_exp = explainer.explain_instance_LIPEx(\n",
    "        sample_data,\n",
    "        sample_labels,\n",
    "        sample_distances,\n",
    "        sample_weights,\n",
    "        used_features=features2use,\n",
    "        new_top_labels=N_labels\n",
    "    )\n",
    "    print('TV_distance',TV(torch.from_numpy(f_output),LIPEx_exp.local_pred))\n",
    "    TV_s.append(TV(torch.from_numpy(f_output),LIPEx_exp.local_pred))\n",
    "\n",
    "print('\\n')\n",
    "print('TV(f(s), LIPEx(s)) =',TV_s)\n",
    "print('len(TV_s)) =',len(TV_s))\n",
    "\n",
    "print('Avg(TV(f(s), LIPEx(s))) =',np.mean(TV_s))\n",
    "print('Std(TV(f(s), LIPEx(s))) =',np.std(TV_s))\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hongbo_lipex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
